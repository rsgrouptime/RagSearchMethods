{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index langchain python-dotenv fastembed qdrant_client sentence_transformers llama-index-llms-mistralai langchain_community ragas\n",
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Union\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "with open(os.path.join(cwd,\"config\",\"global_config.json\")) as f:\n",
    "    gconf = json.load(f)\n",
    "\n",
    "Qdrant_URL = gconf['qdrant_URL']\n",
    "Collection_Name = gconf['Collection_Name'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Documents or Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformation:\n",
    "    def __call__(self, documents):\n",
    "        transformed_documents = []\n",
    "        for doc in documents:\n",
    "            transformed_content = doc.get_content().lower()\n",
    "            transformed_content = re.sub(r'\\s+', ' ', transformed_content)\n",
    "            transformed_content = re.sub(r'[^\\w\\s]', '', transformed_content)\n",
    "            transformed_documents.append(Document(text=transformed_content, metadata=doc.metadata))\n",
    "        return transformed_documents\n",
    "    \n",
    "def Sentence_Splitter_docs_into_nodes(all_documents):\n",
    "    try:\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=int(gconf['chunk_size']),\n",
    "            chunk_overlap=int(gconf['chunk_overlap'])\n",
    "        )\n",
    "\n",
    "        nodes = splitter.get_nodes_from_documents(all_documents)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents into nodes: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_nodes(nodes, output_file):\n",
    "    try:\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        # Convert the TextNode objects to dictionaries\n",
    "        nodes_dict = [node.dict() for node in nodes]\n",
    "\n",
    "        with open(output_file, 'w') as fi:\n",
    "            json.dump(nodes_dict, fi, indent=4)\n",
    "        print(f\"Saved nodes to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving nodes to file: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Load data from directory\n",
    "        documents = SimpleDirectoryReader(input_dir=os.path.join(cwd,gconf['input_directory'])).load_data()\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "        if documents:\n",
    "            # Apply custom transformation\n",
    "            custom_transform = CustomTransformation()\n",
    "            documents = custom_transform(documents)\n",
    "\n",
    "            # Split documents into nodes\n",
    "            nodes = Sentence_Splitter_docs_into_nodes(documents)\n",
    "\n",
    "            sub_chunk_sizes = gconf['sub_chunk_sizes']\n",
    "            sub_node_parsers = [\n",
    "                SimpleNodeParser.from_defaults(chunk_size=c) for c in sub_chunk_sizes\n",
    "            ]\n",
    "            all_nodes = []\n",
    "            for base_node in nodes:\n",
    "                for n in sub_node_parsers:\n",
    "                    sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "                    sub_inodes = [\n",
    "                        IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "                    ]\n",
    "                    all_nodes.extend(sub_inodes)\n",
    "\n",
    "                # also add original node to node\n",
    "                original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "                all_nodes.append(original_node)\n",
    "    \n",
    "            #Save nodes to a single JSON file\n",
    "            print(f\"Created {len(all_nodes)} nodes\")\n",
    "            output_file = os.path.join(cwd,gconf['node_json'])\n",
    "            save_nodes(all_nodes, output_file)\n",
    "\n",
    "        else:\n",
    "            print(\"No documents to process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing documents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Qdrant Collection and Insert Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantIndexing:\n",
    "    \"\"\"\n",
    "    A class for indexing documents using Qdrant vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the QdrantIndexing object.\n",
    "        \"\"\"\n",
    "        self.data_path = os.path.join(cwd,gconf['node_json'])\n",
    "        self.embedding_model = TextEmbedding(model_name=gconf['fastembed_dense_model'])\n",
    "        self.sparse_embedding_model = SparseTextEmbedding(model_name=gconf['fastembed_sparse_model'])\n",
    "        self.qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL)\n",
    "        self.metadata = []\n",
    "        self.documents = []\n",
    "        self.chunkid = []\n",
    "        self.parentid = []\n",
    "        logging.info(\"QdrantIndexing object initialized.\")\n",
    "\n",
    "    def load_nodes(self, input_file):\n",
    "        \"\"\"\n",
    "        Load nodes from a JSON file and extract metadata and documents.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the JSON file.\n",
    "        \"\"\"\n",
    "        with open(input_file, 'r') as file:\n",
    "            self.nodes = json.load(file)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            self.metadata.append(node['metadata'])\n",
    "            self.documents.append(node['text'])\n",
    "            self.chunkid.append(node['id_'])\n",
    "            self.parentid.append(node['index_id'])\n",
    "\n",
    "        logging.info(f\"Loaded {len(self.nodes)} nodes from JSON file.\")\n",
    "\n",
    "    def client_collection(self):\n",
    "        \"\"\"\n",
    "        Create a collection in Qdrant vector database.\n",
    "        \"\"\"\n",
    "        if not self.qdrant_client.collection_exists(collection_name=f\"{Collection_Name}\"): \n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name= Collection_Name,\n",
    "                vectors_config={\n",
    "                     'dense': models.VectorParams(\n",
    "                         size=384,\n",
    "                         distance = models.Distance.COSINE,\n",
    "                     )\n",
    "                },\n",
    "                sparse_vectors_config={\n",
    "                    \"sparse\": models.SparseVectorParams(\n",
    "                              index=models.SparseIndexParams(\n",
    "                                on_disk=False,              \n",
    "                            ),\n",
    "                        )\n",
    "                    }\n",
    "            )\n",
    "            logging.info(f\"Created collection '{Collection_Name}' in Qdrant vector database.\")\n",
    "\n",
    "    def create_sparse_vector(self, text):\n",
    "        \"\"\"\n",
    "        Create a sparse vector from the text using BM42 approach.\n",
    "        \"\"\"\n",
    "        # Generate the sparse vector using BM42\n",
    "        embeddings = list(self.sparse_embedding_model.embed([text]))[0]\n",
    "\n",
    "        # Check if embeddings has indices and values attributes\n",
    "        if hasattr(embeddings, 'indices') and hasattr(embeddings, 'values'):\n",
    "            sparse_vector = models.SparseVector(\n",
    "                indices=embeddings.indices.tolist(),\n",
    "                values=embeddings.values.tolist()\n",
    "            )\n",
    "            return sparse_vector\n",
    "        else:\n",
    "            raise ValueError(\"The embeddings object does not have 'indices' and 'values' attributes.\")\n",
    "\n",
    "\n",
    "    def documents_insertion(self):\n",
    "        points = []\n",
    "        for i, (doc, metadata, chunkid ,parentid) in enumerate(tqdm(zip(self.documents, self.metadata, self.chunkid ,self.parentid), total=len(self.documents))):\n",
    "            # Generate both dense and sparse embeddings\n",
    "            dense_embedding = list(self.embedding_model.embed([doc]))[0]\n",
    "            sparse_vector = self.create_sparse_vector(doc)\n",
    "\n",
    "            # Create PointStruct\n",
    "            point = models.PointStruct(\n",
    "                id=i,\n",
    "                vector={\n",
    "                    'dense': dense_embedding.tolist(),\n",
    "                    'sparse': sparse_vector,\n",
    "                },\n",
    "                payload={\n",
    "                    'text': doc,\n",
    "                    'chunkid':chunkid,\n",
    "                    'parentid':parentid,\n",
    "                    **metadata  # Include all metadata\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        # Upsert points\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=Collection_Name,\n",
    "            points=points\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Upserted {len(points)} points with dense and sparse vectors into Qdrant vector database.\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    indexing = QdrantIndexing()\n",
    "    indexing.load_nodes(indexing.data_path)\n",
    "    indexing.client_collection()\n",
    "    indexing.documents_insertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever Class - Small to Big Retrieval - Parent Child Retriever with hybrid Search Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_search():\n",
    "    \"\"\"\n",
    "    class for performing hybrid search using dense and sparse embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Hybrid_search object with dense and sparse embedding models and a Qdrant client.\n",
    "        \"\"\"\n",
    "        self.embedding_model = TextEmbedding(model_name=gconf['fastembed_dense_model'])\n",
    "        self.sparse_embedding_model = SparseTextEmbedding(model_name=gconf['fastembed_sparse_model'])\n",
    "        self.qdrant_client = QdrantClient(\n",
    "            url=Qdrant_URL,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "    def metadata_filter(self, file_names: Union[str, List[str]]) -> models.Filter:\n",
    "        \n",
    "        if isinstance(file_names, str):\n",
    "            # Single file name\n",
    "            file_name_condition = models.FieldCondition(\n",
    "                key=\"file_name\",\n",
    "                match=models.MatchValue(value=file_names)\n",
    "            )\n",
    "        else:\n",
    "            # List of file names\n",
    "            file_name_condition = models.FieldCondition(\n",
    "                key=\"file_name\",\n",
    "                match=models.MatchAny(any=file_names)\n",
    "            )\n",
    "        \n",
    "        print(\"file_name_condition\",file_name_condition)\n",
    "\n",
    "        return models.Filter(\n",
    "            must=[file_name_condition]\n",
    "        )\n",
    "\n",
    "    def query_hybrid_search_child_chunks(self, query, colname, metadata_filter=None, limit=1):\n",
    "        \n",
    "        # Embed the query using the dense embedding model\n",
    "        dense_query = list(self.embedding_model.embed([query]))[0].tolist()\n",
    "\n",
    "        # Embed the query using the sparse embedding model\n",
    "        sparse_query = list(self.sparse_embedding_model.embed([query]))[0]\n",
    "\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=colname,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=models.SparseVector(indices=sparse_query.indices.tolist(), values=sparse_query.values.tolist()),\n",
    "                    using=\"sparse\",\n",
    "                    limit=limit,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=dense_query,\n",
    "                    using=\"dense\",\n",
    "                    limit=limit,\n",
    "                ),\n",
    "            ],\n",
    "            query_filter=metadata_filter,\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        )\n",
    "        # Extract the document number, score, and text from the payload of each scored point\n",
    "        documents = [{'docum':point.payload['text'],'parent_index':point.payload['parentid']} for point in results.points]\n",
    "\n",
    "        return documents\n",
    "    \n",
    "    def query_hybrid_search_with_Parent_chunks(self, query, parent_index, colname, metadata_filter=None, limit=1):\n",
    "        \n",
    "        # Embed the query using the dense embedding model\n",
    "        dense_query = list(self.embedding_model.embed([query]))[0].tolist()\n",
    "\n",
    "        # Embed the query using the sparse embedding model\n",
    "        sparse_query = list(self.sparse_embedding_model.embed([query]))[0]\n",
    "\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=colname,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=models.SparseVector(indices=sparse_query.indices.tolist(), values=sparse_query.values.tolist()),\n",
    "                    using=\"sparse\",\n",
    "                    limit=limit,filter=models.Filter(must=models.FieldCondition(\n",
    "                    key=\"chunkid\",\n",
    "                    match=models.MatchValue(value=parent_index),\n",
    "                )\n",
    "                )),\n",
    "                models.Prefetch(\n",
    "                    query=dense_query,\n",
    "                    using=\"dense\",\n",
    "                    limit=limit,filter=models.Filter(\n",
    "                must=models.FieldCondition(\n",
    "                    key=\"chunkid\",\n",
    "                    match=models.MatchValue(value=parent_index),\n",
    "                )\n",
    "                )\n",
    "                ),\n",
    "            ],\n",
    "            query_filter=metadata_filter,\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        )\n",
    "        # Extract the document number, score, and text from the payload of each scored point\n",
    "        documents = [point.payload['text'] for point in results.points]\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReRanker Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reranking():\n",
    "    def __init__(self) -> None:\n",
    "        # Load the CrossEncoder model\n",
    "        self.model = CrossEncoder(gconf['reranker_model'])\n",
    "\n",
    "    def rerank_documents(self, query, documents):\n",
    "        # Compute the similarity scores between the query and each document\n",
    "        scores = self.model.predict([(query, doc) for doc in documents])\n",
    "\n",
    "        # Sort the documents based on their similarity scores\n",
    "        ranked_documents = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top 2 documents\n",
    "        top_documents = [doc for doc, score in ranked_documents[:2]]\n",
    "\n",
    "        return top_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prompt_template_generation():\n",
    "    def __init__(self) -> None:\n",
    "        self.Collection_Name = Collection_Name\n",
    "        self.search = Hybrid_search()\n",
    "        self.reranker = reranking()\n",
    "        self.prompt_str = \"\"\"Context: {context_str}\n",
    "\n",
    "                             Analyze the input question to understand its core intent. \\\n",
    "                             Then, search the provided context for key information that directly addresses this intent. \\\n",
    "                             Create an answer that focuses solely on the relevant details found. \\\n",
    "                             Avoid unnecessary complexity and unrelated information to ensure clarity and ease of understanding.\n",
    "            \n",
    "                             Answer Formatting:\n",
    "                              - Please ensure the response is formatted solely as a string.\n",
    "                              - Remove the trailing spaces in the answer.\n",
    "                              - Make sure there will be no duplicate sentences in the answer.\n",
    "            \n",
    "                             Question: {query_str}\n",
    "                             Answer:\n",
    "                            \"\"\"\n",
    "        self.prompt_tmpl = PromptTemplate(template=self.prompt_str, input_variables=[\"context_str\",\"query_str\"])\n",
    "\n",
    "    def prompt_generation(self, query: str, filename: str):\n",
    "        metadata_filter = self.search.metadata_filter(filename)\n",
    "        cont_st_time = datetime.now() \n",
    "        chiunkresults = self.search.query_hybrid_search_child_chunks(query, self.Collection_Name, metadata_filter)\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for summary in chiunkresults:\n",
    "            # For each summary, retrieve relevant detailed chunks\n",
    "            parent_index = summary[\"parent_index\"]\n",
    "            parent_chunks  = self.search.query_hybrid_search_with_Parent_chunks(summary[\"docum\"], parent_index, self.Collection_Name, metadata_filter)\n",
    "            relevant_chunks.extend(parent_chunks)\n",
    "        \n",
    "        cont_ed_time = datetime.now()\n",
    "        cont_time = round((cont_ed_time - cont_st_time).total_seconds(),2)\n",
    "\n",
    "        rank_st_time = datetime.now()\n",
    "        reranked_documents = self.reranker.rerank_documents(query, relevant_chunks)\n",
    "        rank_ed_time = datetime.now()\n",
    "        rank_time = round((rank_ed_time - rank_st_time).total_seconds(),2)\n",
    "        context = \"/n/n\".join(reranked_documents)\n",
    "        \n",
    "        prompt_templ = self.prompt_tmpl\n",
    "\n",
    "        return prompt_templ, context, cont_time, rank_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model():\n",
    "    os.environ['HUGGINGFACEHUB_API_TOKEN'] = gconf['huggingface_token']\n",
    "    hugllm = HuggingFaceHub(repo_id=gconf['hugging_gen_model'],task=gconf['hugging_gen_task'],\n",
    "                                                model_kwargs={\"num_beams\":3,\"top_k\":1,\"temperature\":float(gconf['hugging_gen_temperature']),\n",
    "                                                            \"max_new_tokens\":int(gconf['hugging_gen_max_token'])})\n",
    "    return hugllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Inference - Retrieve first smaller chunks then find its parent index chunk - Pass the retrieved context with query to LLM for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(inputdata):\n",
    "    outobj = prompt_template_generation()\n",
    "    input_pdffiles = os.listdir(os.path.join(cwd,gconf['input_directory']))\n",
    "    prompt_tmpl, contextpmt, cont_time, rank_time = outobj.prompt_generation(query=inputdata, filename=input_pdffiles)\n",
    "\n",
    "    hllm = llm_model()\n",
    "    llm_chain = LLMChain(prompt=prompt_tmpl, \n",
    "                        llm=hllm)\n",
    "\n",
    "    def count_tokens(chain, query, cont):\n",
    "        with get_openai_callback() as cb:\n",
    "            gen_st_time = datetime.now()\n",
    "            resp = chain.run({\"context_str\":cont,\"query_str\":query})\n",
    "            gen_ed_time = datetime.now()\n",
    "            gentime = round((gen_ed_time - gen_st_time).total_seconds(),2)\n",
    "            print(resp)\n",
    "            print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "            print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "            print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "            print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "        \n",
    "        resp = resp.split('Answer:')[-1].replace('\\n',' ').strip()\n",
    "\n",
    "        return resp, gentime\n",
    "    res, gentime = count_tokens(llm_chain, inputdata, contextpmt)\n",
    "\n",
    "    datarec = {\n",
    "        \"question\": [inputdata],\n",
    "        \"answer\": [res],\n",
    "        \"contexts\": [[contextpmt]]\n",
    "    }\n",
    "    return datarec, cont_time, gentime, rank_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test retriever and generation for complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata = \"what is the formula for the Property tax liability?\"\n",
    "#inputdata = \"what is property tax meaning?\"\n",
    "#inputdata = \"Connecticut is not plagued by?\"\n",
    "#inputdata = \"What are the components for determining the property tax liabilities?\"\n",
    "#inputdata = \"What is the homestead value for real property used for Minnesota Analysis?\"\n",
    "#inputdata = \"What are the two cities where assessment limits reduce taxes by 60 percent?\"\n",
    "#inputdata = \"what is the formula to calculate the Net Tax Bill?\"\n",
    "\n",
    "st_time = datetime.now()\n",
    "response, cont_time, gentime, ranktime = inference(inputdata)\n",
    "ed_time = datetime.now()\n",
    "response_time = round((ed_time - st_time).total_seconds(),2)\n",
    "print(\"Generation Response:\",response)\n",
    "print(f\"Context_Retrieval_Latency:    {cont_time} secs\")\n",
    "print(f\"ReRanking_Latency:            {ranktime} secs\")\n",
    "print(f\"RESPONSE_Generation_Latency:  {gentime} secs\")\n",
    "print(f\"Overall_RESPONSE_TIME:        {response_time} secs\")\n",
    "\n",
    "%reset_selective -f hugllm\n",
    "%reset_selective -f hllm\n",
    "%reset_selective -f resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
