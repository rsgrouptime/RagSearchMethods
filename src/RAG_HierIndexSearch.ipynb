{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index langchain python-dotenv fastembed qdrant_client sentence_transformers llama-index-llms-mistralai langchain_community ragas\n",
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from llama_index.core.schema import Document\n",
    "from langchain.docstore.document import Document as doct\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "with open(os.path.join(cwd,\"config\",\"global_config.json\")) as f:\n",
    "    gconf = json.load(f)\n",
    "\n",
    "Qdrant_URL = gconf['qdrant_URL']\n",
    "Collection_Name = gconf['Collection_Name'][2]\n",
    "SumCollection_Name = gconf['Collection_Name'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Detailed child Documents and their summaries for hierarchical indexing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformation:\n",
    "    def __call__(self, documents):\n",
    "        transformed_documents = []\n",
    "        for doc in documents:\n",
    "            transformed_content = doc.get_content().lower()\n",
    "            transformed_content = re.sub(r'\\s+', ' ', transformed_content)\n",
    "            transformed_content = re.sub(r'[^\\w\\s]', '', transformed_content)\n",
    "            transformed_documents.append(Document(text=transformed_content, metadata=doc.metadata))\n",
    "        return transformed_documents\n",
    "    \n",
    "def Sentence_Splitter_docs_into_nodes(all_documents):\n",
    "    try:\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=int(gconf['chunk_size']),\n",
    "            chunk_overlap=int(gconf['chunk_overlap'])\n",
    "        )\n",
    "\n",
    "        nodes = splitter.get_nodes_from_documents(all_documents)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents into nodes: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_nodes(nodes, output_file):\n",
    "    try:\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        # Convert the TextNode objects to dictionaries\n",
    "        nodes_dict = [node.dict() for node in nodes]\n",
    "\n",
    "        with open(output_file, 'w') as file:\n",
    "            json.dump(nodes_dict, file, indent=4)\n",
    "        print(f\"Saved nodes to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving nodes to file: {e}\")\n",
    "async def exponential_backoff(attempt):\n",
    "    \"\"\"\n",
    "    Implements exponential backoff with a jitter.\n",
    "    \n",
    "    Args:\n",
    "        attempt: The current retry attempt number.\n",
    "        \n",
    "    Waits for a period of time before retrying the operation.\n",
    "    The wait time is calculated as (2^attempt) + a random fraction of a second.\n",
    "    \"\"\"\n",
    "    # Calculate the wait time with exponential backoff and jitter\n",
    "    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "    print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "\n",
    "    # Asynchronously sleep for the calculated wait time\n",
    "    await asyncio.sleep(wait_time)\n",
    "\n",
    "\n",
    "async def retry_with_exponential_backoff(coroutine, max_retries=1):\n",
    "    \"\"\"\n",
    "    Retries a coroutine using exponential backoff upon encountering a RateLimitError.\n",
    "    \n",
    "    Args:\n",
    "        coroutine: The coroutine to be executed.\n",
    "        max_retries: The maximum number of retry attempts.\n",
    "        \n",
    "    Returns:\n",
    "        The result of the coroutine if successful.\n",
    "        \n",
    "    Raises:\n",
    "        The last encountered exception if all retry attempts fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Attempt to execute the coroutine\n",
    "            return await coroutine\n",
    "        except Exception as e:\n",
    "            # If the last attempt also fails, raise the exception\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "\n",
    "            # Wait for an exponential backoff period before retrying\n",
    "            await exponential_backoff(attempt)\n",
    "\n",
    "    # If max retries are reached without success, raise an exception\n",
    "    raise Exception(\"Max retries reached\")\n",
    "\n",
    "async def summarize_doc(docs,i):\n",
    "    final_docs = doct(\n",
    "        page_content=docs.text,\n",
    "        metadata=docs.metadata\n",
    "    )\n",
    "    i = asyncio.create_task(retry_with_exponential_backoff(summary_chain.ainvoke([final_docs])))\n",
    "    await i\n",
    "    summary = i.result()\n",
    "    summary = summary['output_text']\n",
    "    summary = summary.split(\"\\n\\n\\nCONCISE SUMMARY:\")[1].strip().replace('\\n',' ')\n",
    "    return doct(\n",
    "        page_content=summary,\n",
    "        metadata={\"source\": docs.metadata[\"file_path\"], \"page\": docs.metadata[\"page_label\"], \"summary\": True}\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Load data from directory\n",
    "        documents = SimpleDirectoryReader(input_dir=os.path.join(cwd,gconf['input_directory'])).load_data()\n",
    "        print(f\"Loaded {len(documents)} documents {documents}\")\n",
    "        \n",
    "        if documents:\n",
    "            # Apply custom transformation\n",
    "            custom_transform = CustomTransformation()\n",
    "            documents = custom_transform(documents)\n",
    "        \n",
    "            # Split documents into nodes\n",
    "            nodes = Sentence_Splitter_docs_into_nodes(documents)\n",
    "        \n",
    "            print(f\"Created {len(nodes)} nodes\")\n",
    "\n",
    "            # Save nodes to a single JSON file\n",
    "            output_file = os.path.join(cwd,gconf['node_json'])\n",
    "            save_nodes(nodes, output_file)\n",
    "\n",
    "            # Create document-level summaries\n",
    "            os.environ['HUGGINGFACEHUB_API_TOKEN'] = gconf['huggingface_token']\n",
    "            summary_llm = HuggingFaceHub(repo_id=gconf['hugging_gen_model'],task=gconf['hugging_gen_task'],\n",
    "                                        model_kwargs={\"temperature\":float(gconf['hugging_gen_temperature']),\n",
    "                                                      \"max_new_tokens\":int(gconf['hugging_gen_max_token'])})\n",
    "            summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "            \n",
    "            batch_size = 5  #Adjust this based on your rate limits\n",
    "            summaries = []\n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch = documents[i:i+batch_size]\n",
    "                batch_summaries = await asyncio.gather(*[summarize_doc(doc,i) for doc in batch])\n",
    "                summaries.extend(batch_summaries)\n",
    "                await asyncio.sleep(1)  # Short pause between batches\n",
    "            print(\"Summaries\",summaries)\n",
    "\n",
    "        else:\n",
    "            print(\"No documents to process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing documents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Qdrant Collections and Insert Documents with their summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantIndexing:\n",
    "    \"\"\"\n",
    "    A class for indexing documents using Qdrant vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the QdrantIndexing object.\n",
    "        \"\"\"\n",
    "        self.data_path = os.path.join(cwd,gconf['node_json'])\n",
    "        self.embedding_model = TextEmbedding(model_name=gconf['fastembed_dense_model'])\n",
    "        self.sparse_embedding_model = SparseTextEmbedding(model_name=gconf['fastembed_sparse_model'])\n",
    "        self.qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL)\n",
    "        self.metadata = []\n",
    "        self.documents = []\n",
    "        self.metadata_sum = []\n",
    "        self.documents_sum = []\n",
    "        logging.info(\"QdrantIndexing object initialized.\")\n",
    "\n",
    "    def load_nodes(self, input_file):\n",
    "        \"\"\"\n",
    "        Load nodes from a JSON file and extract metadata and documents.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the JSON file.\n",
    "        \"\"\"\n",
    "        with open(input_file, 'r') as file:\n",
    "            self.nodes = json.load(file)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            self.metadata.append(node['metadata'])\n",
    "            self.documents.append(node['text'])\n",
    "        \n",
    "        for nodesum in summaries:\n",
    "            self.documents_sum.append(nodesum.page_content)\n",
    "        \n",
    "        logging.info(f\"Loaded {len(self.nodes)} nodes from JSON file.\")\n",
    "\n",
    "    def client_collection(self):\n",
    "        \"\"\"\n",
    "        Create a collection in Qdrant vector database.\n",
    "        \"\"\"\n",
    "        bothcollections = [Collection_Name,SumCollection_Name]\n",
    "        for col in bothcollections:\n",
    "            if not self.qdrant_client.collection_exists(collection_name=f\"{col}\"): \n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name= col,\n",
    "                    vectors_config={\n",
    "                        'dense': models.VectorParams(\n",
    "                            size=384,\n",
    "                            distance = models.Distance.COSINE,\n",
    "                        )\n",
    "                    },\n",
    "                    sparse_vectors_config={\n",
    "                        \"sparse\": models.SparseVectorParams(\n",
    "                                index=models.SparseIndexParams(\n",
    "                                    on_disk=False,              \n",
    "                                ),\n",
    "                            )\n",
    "                        }\n",
    "                )\n",
    "                logging.info(f\"Created collection '{col}' in Qdrant vector database.\")\n",
    "\n",
    "    def create_sparse_vector(self, text):\n",
    "        \"\"\"\n",
    "        Create a sparse vector from the text using BM42 approach.\n",
    "        \"\"\"\n",
    "        # Generate the sparse vector using BM42\n",
    "        embeddings = list(self.sparse_embedding_model.embed([text]))[0]\n",
    "\n",
    "        # Check if embeddings has indices and values attributes\n",
    "        if hasattr(embeddings, 'indices') and hasattr(embeddings, 'values'):\n",
    "            sparse_vector = models.SparseVector(\n",
    "                indices=embeddings.indices.tolist(),\n",
    "                values=embeddings.values.tolist()\n",
    "            )\n",
    "            return sparse_vector\n",
    "        else:\n",
    "            raise ValueError(\"The embeddings object does not have 'indices' and 'values' attributes.\")\n",
    "\n",
    "\n",
    "    def documents_insertion(self):\n",
    "        points = []\n",
    "        sumpoints =[]\n",
    "        for i, (doc, metadata) in enumerate(tqdm(zip(self.documents, self.metadata), total=len(self.documents))):\n",
    "            # Generate both dense and sparse embeddings\n",
    "            dense_embedding = list(self.embedding_model.embed([doc]))[0]\n",
    "            sparse_vector = self.create_sparse_vector(doc)\n",
    "\n",
    "            # Create PointStruct\n",
    "            point = models.PointStruct(\n",
    "                id=i,\n",
    "                vector={\n",
    "                    'dense': dense_embedding.tolist(),\n",
    "                    'sparse': sparse_vector,\n",
    "                },\n",
    "                payload={\n",
    "                    'text': doc,\n",
    "                    **metadata  # Include all metadata\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Upsert points\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=Collection_Name,\n",
    "            points=points\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Upserted {len(points)} points with dense and sparse vectors into Qdrant vector database.\")\n",
    "\n",
    "        for j, (docsum, metadata) in enumerate(tqdm(zip(self.documents_sum, self.metadata), total=len(self.documents_sum))):\n",
    "            # Generate both dense and sparse embeddings\n",
    "            dense_embeddingsum = list(self.embedding_model.embed([docsum]))[0]\n",
    "            sparse_vectorsum = self.create_sparse_vector(docsum)\n",
    "\n",
    "            # Create PointStruct\n",
    "            pointsum = models.PointStruct(\n",
    "                id=j,\n",
    "                vector={\n",
    "                    'dense': dense_embeddingsum.tolist(),\n",
    "                    'sparse': sparse_vectorsum,\n",
    "                },\n",
    "                payload={\n",
    "                    'text': docsum,\n",
    "                    **metadata  # Include all metadata\n",
    "                }\n",
    "            )\n",
    "            sumpoints.append(pointsum)\n",
    "\n",
    "        # Upsert sumpoints\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=SumCollection_Name,\n",
    "            points=sumpoints\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Upserted {len(points)} points with dense and sparse vectors into Qdrant vector database.\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    indexing = QdrantIndexing()\n",
    "    indexing.load_nodes(indexing.data_path)\n",
    "    indexing.client_collection()\n",
    "    indexing.documents_insertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever Class - Hierarchical Indexing Technique with hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_search():\n",
    "    \"\"\"\n",
    "    class for performing hybrid search using dense and sparse embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Hybrid_search object with dense and sparse embedding models and a Qdrant client.\n",
    "        \"\"\"\n",
    "        self.embedding_model = TextEmbedding(model_name=gconf['fastembed_dense_model'])\n",
    "        self.sparse_embedding_model = SparseTextEmbedding(model_name=gconf['fastembed_sparse_model'])\n",
    "        self.qdrant_client = QdrantClient(\n",
    "            url=Qdrant_URL,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "    def metadata_filter(self, file_names: Union[str, List[str]]) -> models.Filter:\n",
    "        \n",
    "        if isinstance(file_names, str):\n",
    "            # Single file name\n",
    "            file_name_condition = models.FieldCondition(\n",
    "                key=\"file_name\",\n",
    "                match=models.MatchValue(value=file_names)\n",
    "            )\n",
    "        else:\n",
    "            # List of file names\n",
    "            file_name_condition = models.FieldCondition(\n",
    "                key=\"file_name\",\n",
    "                match=models.MatchAny(any=file_names)\n",
    "            )\n",
    "        \n",
    "        print(\"file_name_condition\",file_name_condition)\n",
    "\n",
    "        return models.Filter(\n",
    "            must=[file_name_condition]\n",
    "        )\n",
    "\n",
    "    def query_hybrid_search(self, query, colname, metadata_filter=None, limit=1):\n",
    "        \n",
    "        # Embed the query using the dense embedding model\n",
    "        dense_query = list(self.embedding_model.embed([query]))[0].tolist()\n",
    "\n",
    "        # Embed the query using the sparse embedding model\n",
    "        sparse_query = list(self.sparse_embedding_model.embed([query]))[0]\n",
    "\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=colname,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=models.SparseVector(indices=sparse_query.indices.tolist(), values=sparse_query.values.tolist()),\n",
    "                    using=\"sparse\",\n",
    "                    limit=limit,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=dense_query,\n",
    "                    using=\"dense\",\n",
    "                    limit=limit,\n",
    "                ),\n",
    "            ],\n",
    "            query_filter=metadata_filter,\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        )\n",
    "        # Extract the document number, score, and text from the payload of each scored point\n",
    "        documents = [{'docum':point.payload['text'],'page':point.payload['page_label']} for point in results.points]\n",
    "\n",
    "        return documents\n",
    "    \n",
    "    def query_hybrid_search_with_filter(self, query, pg_num, colname, metadata_filter=None, limit=1):\n",
    "        \n",
    "        # Embed the query using the dense embedding model\n",
    "        dense_query = list(self.embedding_model.embed([query]))[0].tolist()\n",
    "\n",
    "        # Embed the query using the sparse embedding model\n",
    "        sparse_query = list(self.sparse_embedding_model.embed([query]))[0]\n",
    "\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=colname,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=models.SparseVector(indices=sparse_query.indices.tolist(), values=sparse_query.values.tolist()),\n",
    "                    using=\"sparse\",\n",
    "                    limit=limit,filter=models.Filter(must=models.FieldCondition(\n",
    "                    key=\"page_label\",\n",
    "                    match=models.MatchValue(value=pg_num),\n",
    "                )\n",
    "                )),\n",
    "                models.Prefetch(\n",
    "                    query=dense_query,\n",
    "                    using=\"dense\",\n",
    "                    limit=limit,filter=models.Filter(\n",
    "                must=models.FieldCondition(\n",
    "                    key=\"page_label\",\n",
    "                    match=models.MatchValue(value=pg_num),\n",
    "                )\n",
    "                )\n",
    "                ),\n",
    "            ],\n",
    "            query_filter=metadata_filter,\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        )\n",
    "        # Extract the document number, score, and text from the payload of each scored point\n",
    "        documents = [point.payload['text'] for point in results.points]\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReRanker Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reranking():\n",
    "    def __init__(self) -> None:\n",
    "        # Load the CrossEncoder model\n",
    "        self.model = CrossEncoder(gconf['reranker_model'])\n",
    "\n",
    "    def rerank_documents(self, query, documents):\n",
    "        # Compute the similarity scores between the query and each document\n",
    "        scores = self.model.predict([(query, doc) for doc in documents])\n",
    "\n",
    "        # Sort the documents based on their similarity scores\n",
    "        ranked_documents = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top 2 documents\n",
    "        top_documents = [doc for doc, score in ranked_documents[:2]]\n",
    "\n",
    "        return top_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prompt_template_generation():\n",
    "    def __init__(self) -> None:\n",
    "        self.Collection_Name = Collection_Name\n",
    "        self.SumCollection_Name = SumCollection_Name\n",
    "        self.search = Hybrid_search()\n",
    "        self.reranker = reranking()\n",
    "        self.prompt_str = \"\"\"You are an AI assistant specializing in explaining complex topics related to Retrieval-Augmented Generation(RAG). Your task is to provide a clear, concise, and informative explanation based on the following context and query.\n",
    "\n",
    "        Context:\n",
    "        {context_str}\n",
    "\n",
    "        Query: {query_str}\n",
    "\n",
    "        Please follow these guidelines in your response:\n",
    "        1. Start with a brief overview of the concept mentioned in the query.\n",
    "        2. Provide at least one concrete example or use case to illustrate the concept.\n",
    "        3. If there are any limitations or challenges associated with this concept, briefly mention them.\n",
    "        4. Conclude with a sentence about the potential future impact or applications of this concept.\n",
    "\n",
    "        Response: Your explanation should be informative yet accessible, suitable for someone with a basic understanding of RAG. If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer, and only respond based on the given context.\n",
    "        Output: Remember output should be only the predicted answer.\n",
    "        \"\"\"\n",
    "        self.prompt_tmpl = PromptTemplate(template=self.prompt_str, input_variables=[\"context_str\",\"query_str\"])\n",
    "\n",
    "    def prompt_generation(self, query: str, filename: str):\n",
    "        metadata_filter = self.search.metadata_filter(filename)\n",
    "        cont_st_time = datetime.now() \n",
    "        sumresults = self.search.query_hybrid_search(query, self.SumCollection_Name, metadata_filter)\n",
    "\n",
    "        relevant_chunks = []\n",
    "        for summary in sumresults:\n",
    "            # For each summary, retrieve relevant detailed chunks\n",
    "            page_number = summary[\"page\"]\n",
    "            page_chunks  = self.search.query_hybrid_search_with_filter(summary[\"docum\"], page_number, self.Collection_Name, metadata_filter)\n",
    "            relevant_chunks.extend(page_chunks)\n",
    "        \n",
    "        cont_ed_time = datetime.now()\n",
    "        cont_time = round((cont_ed_time - cont_st_time).total_seconds(),2)\n",
    "\n",
    "        rank_st_time = datetime.now()\n",
    "        reranked_documents = self.reranker.rerank_documents(query, relevant_chunks)\n",
    "        rank_ed_time = datetime.now()\n",
    "        rank_time = round((rank_ed_time - rank_st_time).total_seconds(),2)\n",
    "        context = \"/n/n\".join(reranked_documents)\n",
    "        \n",
    "        prompt_templ = self.prompt_tmpl\n",
    "\n",
    "        return prompt_templ, context, cont_time, rank_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model():\n",
    "    os.environ['HUGGINGFACEHUB_API_TOKEN'] = gconf['huggingface_token']\n",
    "    hugllm = HuggingFaceHub(repo_id=gconf['hugging_gen_model'],task=gconf['hugging_gen_task'],\n",
    "                                                model_kwargs={\"num_beams\":3,\"top_k\":1,\"temperature\":float(gconf['hugging_gen_temperature']),\n",
    "                                                            \"max_new_tokens\":int(gconf['hugging_gen_max_token'])})\n",
    "    return hugllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(inputdata):\n",
    "    outobj = prompt_template_generation()\n",
    "    input_pdffiles = os.listdir(os.path.join(cwd,gconf['input_directory']))\n",
    "    prompt_tmpl, contextpmt, cont_time, rank_time = outobj.prompt_generation(query=inputdata, filename=input_pdffiles)\n",
    "\n",
    "    hllm = llm_model()\n",
    "    llm_chain = LLMChain(prompt=prompt_tmpl, \n",
    "                        llm=hllm)\n",
    "\n",
    "    def count_tokens(chain, query, cont):\n",
    "        with get_openai_callback() as cb:\n",
    "            gen_st_time = datetime.now()\n",
    "            resp = chain.run({\"context_str\":cont,\"query_str\":query})\n",
    "            gen_ed_time = datetime.now()\n",
    "            gentime = round((gen_ed_time - gen_st_time).total_seconds(),2)\n",
    "            print(resp)\n",
    "            print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "            print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "            print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "            print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "        \n",
    "        resp = resp.split('\\n        \\n')[-1].replace('\\n',' ')\n",
    "\n",
    "        return resp, gentime\n",
    "    res, gentime = count_tokens(llm_chain, inputdata, contextpmt)\n",
    "\n",
    "    datarec = {\n",
    "        \"question\": [inputdata],\n",
    "        \"answer\": [res],\n",
    "        \"contexts\": [[contextpmt]]\n",
    "    }\n",
    "    return datarec, cont_time, gentime, rank_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test retriever and generation for complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputdata = \"what is the formula for Property tax liability?\"\n",
    "#inputdata = \"what is property tax meaning?\"\n",
    "#inputdata = \"Connecticut is not plagued by?\"\n",
    "#inputdata = \"What are the components for determining the property tax liabilities?\"\n",
    "#inputdata = \"What is the homestead value for real property used for Minnesota Analysis?\"\n",
    "#inputdata = \"What are the two cities where assessment limits reduce taxes by 60 percent?\"\n",
    "inputdata = \"what is the formula to calculate the Net Tax Bill?\"\n",
    "\n",
    "st_time = datetime.now()\n",
    "response, cont_time, gentime, ranktime = inference(inputdata)\n",
    "ed_time = datetime.now()\n",
    "response_time = round((ed_time - st_time).total_seconds(),2)\n",
    "print(\"Generation Response:\",response)\n",
    "print(f\"Context_Retrieval_Latency:    {cont_time} secs\")\n",
    "print(f\"ReRanking_Latency:            {ranktime} secs\")\n",
    "print(f\"RESPONSE_Generation_Latency:  {gentime} secs\")\n",
    "print(f\"Overall_RESPONSE_TIME:        {response_time} secs\")\n",
    "\n",
    "%reset_selective -f hugllm\n",
    "%reset_selective -f hllm\n",
    "%reset_selective -f resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
