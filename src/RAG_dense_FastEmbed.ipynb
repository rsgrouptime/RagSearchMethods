{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (0.11.17)\n",
      "Requirement already satisfied: langchain in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: fastembed in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: qdrant-client in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: llama-index-llms-mistralai in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (0.2.6)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ragas in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.4 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.3.4)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.17 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.11.18)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.10 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.2.13)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.2.2)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.2.2)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (0.1.135)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: PyStemmer<3.0.0,>=2.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (2.2.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (0.25.2)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (0.7.2)\n",
      "Requirement already satisfied: mmh3<5.0,>=4.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (4.1.0)\n",
      "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (1.17.0)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (1.19.2)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (10.4.0)\n",
      "Requirement already satisfied: snowballstemmer<3.0.0,>=2.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (2.2.0)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (0.20.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from fastembed) (4.66.5)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from qdrant-client) (1.67.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.2)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from qdrant-client) (2.2.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from sentence-transformers) (4.45.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: mistralai>=1.0.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-llms-mistralai) (1.1.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain-community) (2.5.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from ragas) (3.0.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from ragas) (0.8.0)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from ragas) (0.2.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from ragas) (1.51.2)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.28.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.1.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.0.8)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (3.4.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.7)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (1.1.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (0.2.0)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from python-dateutil==2.8.2->mistralai>=1.0.0->llama-index-llms-mistralai) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (24.3.25)\n",
      "Requirement already satisfied: sympy in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (1.13.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from openai>1->ragas) (0.6.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (307)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from datasets->ragas) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed) (3.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from bitsandbytes) (2.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from torch->bitsandbytes) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sarpa\\anaconda3\\envs\\nttenv\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index langchain python-dotenv fastembed qdrant-client sentence-transformers llama-index-llms-mistralai langchain-community ragas\n",
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "with open(os.path.join(cwd,\"config\",\"global_config.json\")) as f:\n",
    "    gconf = json.load(f)\n",
    "\n",
    "Qdrant_URL = gconf['qdrant_URL']\n",
    "Collection_Name = gconf['Collection_Name'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Documents or Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformation:\n",
    "    def __call__(self, documents):\n",
    "        transformed_documents = []\n",
    "        for doc in documents:\n",
    "            transformed_content = doc.get_content().lower()\n",
    "            transformed_content = re.sub(r'\\s+', ' ', transformed_content)\n",
    "            transformed_content = re.sub(r'[^\\w\\s]', '', transformed_content)\n",
    "            transformed_documents.append(Document(text=transformed_content, metadata=doc.metadata))\n",
    "        return transformed_documents\n",
    "    \n",
    "def Sentence_Splitter_docs_into_nodes(all_documents):\n",
    "    try:\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=int(gconf['chunk_size']),\n",
    "            chunk_overlap=int(gconf['chunk_overlap'])\n",
    "        )\n",
    "\n",
    "        nodes = splitter.get_nodes_from_documents(all_documents)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents into nodes: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_nodes(nodes, output_file):\n",
    "    try:\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        # Convert the TextNode objects to dictionaries\n",
    "        nodes_dict = [node.dict() for node in nodes]\n",
    "\n",
    "        with open(output_file, 'w') as file:\n",
    "            json.dump(nodes_dict, file, indent=4)\n",
    "        print(f\"Saved nodes to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving nodes to file: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Load data from directory\n",
    "        documents = SimpleDirectoryReader(input_dir=os.path.join(cwd,gconf['input_directory'])).load_data()\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "        \n",
    "        if documents:\n",
    "            # Apply custom transformation\n",
    "            custom_transform = CustomTransformation()\n",
    "            documents = custom_transform(documents)\n",
    "\n",
    "            # Split documents into nodes\n",
    "            nodes = Sentence_Splitter_docs_into_nodes(documents)\n",
    "\n",
    "            print(f\"Created {len(nodes)} nodes\")\n",
    "\n",
    "            # Save nodes to a single JSON file\n",
    "            output_file = os.path.join(cwd,gconf['node_json'])\n",
    "            save_nodes(nodes, output_file)\n",
    "\n",
    "        else:\n",
    "            print(\"No documents to process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing documents: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Qdrant Collection and Insert Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantIndexing:\n",
    "    \"\"\"\n",
    "    A class for indexing documents using Qdrant vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the QdrantIndexing object.\n",
    "        \"\"\"\n",
    "        self.data_path = os.path.join(cwd,gconf['node_json'])\n",
    "        self.embedding_model = TextEmbedding(model_name=gconf['fastembed_dense_model'])\n",
    "        self.qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL)\n",
    "        self.metadata = []\n",
    "        self.documents = []\n",
    "        logging.info(\"QdrantIndexing object initialized.\")\n",
    "\n",
    "    def load_nodes(self, input_file):\n",
    "        \"\"\"\n",
    "        Load nodes from a JSON file and extract metadata and documents.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the JSON file.\n",
    "        \"\"\"\n",
    "        with open(input_file, 'r') as file:\n",
    "            self.nodes = json.load(file)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            self.metadata.append(node['metadata'])\n",
    "            self.documents.append(node['text'])\n",
    "\n",
    "        logging.info(f\"Loaded {len(self.nodes)} nodes from JSON file.\")\n",
    "\n",
    "    def client_collection(self):\n",
    "        \"\"\"\n",
    "        Create a collection in Qdrant vector database.\n",
    "        \"\"\"\n",
    "        if not self.qdrant_client.collection_exists(collection_name=f\"{Collection_Name}\"): \n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name= Collection_Name,\n",
    "                vectors_config={\n",
    "                     'dense': models.VectorParams(\n",
    "                         size=384,\n",
    "                         distance = models.Distance.COSINE,\n",
    "                     )\n",
    "                }\n",
    "            )\n",
    "            logging.info(f\"Created collection '{Collection_Name}' in Qdrant vector database.\")\n",
    "\n",
    "    def documents_insertion(self):\n",
    "        points = []\n",
    "        for i, (doc, metadata) in enumerate(tqdm(zip(self.documents, self.metadata), total=len(self.documents))):\n",
    "            # Generate both dense embeddings\n",
    "            dense_embedding = list(self.embedding_model.embed([doc]))[0]\n",
    "\n",
    "            # Create PointStruct\n",
    "            point = models.PointStruct(\n",
    "                id=i,\n",
    "                vector={\n",
    "                    'dense': dense_embedding.tolist()\n",
    "                },\n",
    "                payload={\n",
    "                    'text': doc,\n",
    "                    **metadata  # Include all metadata\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        # Upsert points\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=Collection_Name,\n",
    "            points=points\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Upserted {len(points)} points with dense vectors into Qdrant vector database.\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    indexing = QdrantIndexing()\n",
    "    indexing.load_nodes(indexing.data_path)\n",
    "    indexing.client_collection()\n",
    "    indexing.documents_insertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever Class - Dense Search Technique with Fastembed Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_search():\n",
    "    \"\"\"\n",
    "    class for performing dense search using dense embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Dense_search object with dense embedding models and a Qdrant client.\n",
    "        \"\"\"\n",
    "        self.embedding_model = TextEmbedding(model_name=gconf['fastembed_dense_model'])\n",
    "        self.qdrant_client = QdrantClient(\n",
    "            url=Qdrant_URL,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "    def metadata_filter(self, file_names: Union[str, List[str]]) -> models.Filter:\n",
    "        \n",
    "        if isinstance(file_names, str):\n",
    "            # Single file name\n",
    "            file_name_condition = models.FieldCondition(\n",
    "                key=\"file_name\",\n",
    "                match=models.MatchValue(value=file_names)\n",
    "            )\n",
    "        else:\n",
    "            # List of file names\n",
    "            file_name_condition = models.FieldCondition(\n",
    "                key=\"file_name\",\n",
    "                match=models.MatchAny(any=file_names)\n",
    "            )\n",
    "        print(\"file_name_condition\",file_name_condition)\n",
    "        return models.Filter(\n",
    "            must=[file_name_condition]\n",
    "        )\n",
    "\n",
    "    def query_dense_search(self, query, metadata_filter=None, limit=5):\n",
    "        \n",
    "        # Embed the query using the dense embedding model\n",
    "        dense_query = list(self.embedding_model.embed([query]))[0].tolist()\n",
    "        print(dense_query)\n",
    "\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=Collection_Name,\n",
    "            using=\"dense\",\n",
    "            limit=limit,\n",
    "            query_filter=metadata_filter,\n",
    "            query=dense_query\n",
    "        )\n",
    "        \n",
    "        # Extract the document number, score, and text from the payload of each scored point\n",
    "        documents = [point.payload['text'] for point in results.points]\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReRanker Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reranking():\n",
    "    def __init__(self) -> None:\n",
    "        # Load the CrossEncoder model\n",
    "        self.model = CrossEncoder(gconf['reranker_model'])\n",
    "\n",
    "    def rerank_documents(self, query, documents):\n",
    "        # Compute the similarity scores between the query and each document\n",
    "        scores = self.model.predict([(query, doc) for doc in documents])\n",
    "\n",
    "        # Sort the documents based on their similarity scores\n",
    "        ranked_documents = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top 2 documents\n",
    "        top_documents = [doc for doc, score in ranked_documents[:2]]\n",
    "\n",
    "        return top_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prompt_template_generation():\n",
    "    def __init__(self) -> None:\n",
    "        self.search = Dense_search()\n",
    "        self.reranker = reranking()\n",
    "        self.prompt_str = \"\"\"Context: {context_str}\n",
    "\n",
    "                             Analyze the input question to understand its core intent. \\\n",
    "                             Then, search the provided context for key information that directly addresses this intent. \\\n",
    "                             Create an answer that focuses solely on the relevant details found. \\\n",
    "                             Avoid unnecessary complexity and unrelated information to ensure clarity and ease of understanding.\n",
    "            \n",
    "                             Answer Formatting:\n",
    "                              - Please ensure the response is formatted solely as a string.\n",
    "                              - Remove the trailing spaces in the answer.\n",
    "                              - Make sure there will be no duplicate sentences in the answer.\n",
    "            \n",
    "                             Question: {query_str}\n",
    "                             Answer:\n",
    "                            \"\"\"\n",
    "        self.prompt_tmpl = PromptTemplate(template=self.prompt_str, input_variables=[\"context_str\",\"query_str\"])\n",
    "\n",
    "    def prompt_generation(self, query: str, filename: str):\n",
    "        metadata_filter = self.search.metadata_filter(filename)\n",
    "        cont_st_time = datetime.now() \n",
    "        results = self.search.query_dense_search(query, metadata_filter)\n",
    "        cont_ed_time = datetime.now()\n",
    "        cont_time = round((cont_ed_time - cont_st_time).total_seconds(),2)\n",
    "\n",
    "        rank_st_time = datetime.now() \n",
    "        reranked_documents = self.reranker.rerank_documents(query, results)\n",
    "        rank_ed_time = datetime.now()\n",
    "        rank_time = round((rank_ed_time - rank_st_time).total_seconds(),2)\n",
    "        \n",
    "        context = \"/n/n\".join(reranked_documents)\n",
    "        \n",
    "        prompt_templ = self.prompt_tmpl\n",
    "\n",
    "        return prompt_templ, context, cont_time, rank_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model():\n",
    "    os.environ['HUGGINGFACEHUB_API_TOKEN'] = gconf['huggingface_token']\n",
    "    hugllm = HuggingFaceHub(repo_id=gconf['hugging_gen_model'],task=gconf['hugging_gen_task'],\n",
    "                                                model_kwargs={\"num_beams\":3,\"top_k\":1,\"temperature\":float(gconf['hugging_gen_temperature']),\n",
    "                                                            \"max_new_tokens\":int(gconf['hugging_gen_max_token'])})\n",
    "    return hugllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(inputdata):\n",
    "    outobj = prompt_template_generation()\n",
    "\n",
    "    input_pdffiles = os.listdir(os.path.join(cwd,gconf['input_directory']))\n",
    "    prompt_tmpl, contextpmt, cont_time, rank_time = outobj.prompt_generation(query=inputdata, filename=input_pdffiles)\n",
    "\n",
    "    hllm = llm_model()\n",
    "    llm_chain = LLMChain(prompt=prompt_tmpl, \n",
    "                        llm=hllm)\n",
    "\n",
    "    def count_tokens(chain, query, cont):\n",
    "        with get_openai_callback() as cb:\n",
    "            gen_st_time = datetime.now()\n",
    "            resp = chain.run({\"context_str\":cont,\"query_str\":query})\n",
    "            gen_ed_time = datetime.now()\n",
    "            gentime = round((gen_ed_time - gen_st_time).total_seconds(),2)\n",
    "            print(resp)\n",
    "            print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "            print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "            print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "            print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "        \n",
    "        resp = resp.split('Answer:')[-1].replace('\\n',' ').strip()\n",
    "\n",
    "        return resp, gentime\n",
    "    res, gentime = count_tokens(llm_chain, inputdata, contextpmt)\n",
    "\n",
    "    datarec = {\n",
    "        \"question\": [inputdata],\n",
    "        \"answer\": [res],\n",
    "        \"contexts\": [[contextpmt]]\n",
    "    }\n",
    "    return datarec, cont_time, gentime, rank_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test retriever and generation for complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata = \"what is the formula for the Property tax liability?\"\n",
    "#inputdata = \"what is property tax meaning?\"\n",
    "#inputdata = \"Connecticut is not plagued by?\"\n",
    "#inputdata = \"What are the components for determining the property tax liabilities?\"\n",
    "#inputdata = \"What is the homestead value for real property used for Minnesota Analysis?\"\n",
    "#inputdata = \"What are the two cities where assessment limits reduce taxes by 60 percent?\"\n",
    "#inputdata = \"what is the formula to calculate the Net Tax Bill?\"\n",
    "\n",
    "st_time = datetime.now()\n",
    "response, cont_time, gentime, ranktime = inference(inputdata)\n",
    "ed_time = datetime.now()\n",
    "response_time = round((ed_time - st_time).total_seconds(),2)\n",
    "print(\"Generation Response:\",response)\n",
    "print(f\"Context_Retrieval_Latency:    {cont_time} secs\")\n",
    "print(f\"ReRanking_Latency:            {ranktime} secs\")\n",
    "print(f\"RESPONSE_Generation_Latency:  {gentime} secs\")\n",
    "print(f\"Overall_RESPONSE_TIME:        {response_time} secs\")\n",
    "\n",
    "%reset_selective -f hugllm\n",
    "%reset_selective -f hllm\n",
    "%reset_selective -f resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
